#!/usr/bin/env python3
"""
Universal Model Merger V2 for FusionBench
æ–°ç‰ˆæœ¬ï¼šå…ˆå¯¹ç•™ä¸‹çš„groupå¹³å‡ï¼ˆä¸è®¡ç®—0ï¼‰ï¼Œç„¶åå†ä¹˜ä»¥alpha
"""

import logging
from typing import Dict, Literal, Union
import gc

import torch
from torch import nn
from tqdm import tqdm

from fusion_bench.method.base_algorithm import BaseAlgorithm
from fusion_bench.mixins import SimpleProfilerMixin
from fusion_bench.modelpool import BaseModelPool

log = logging.getLogger(__name__)

__all__ = ["UniversalMergerV2Algorithm"]


class UniversalMergerV2Algorithm(SimpleProfilerMixin, BaseAlgorithm):
    """
    Universal Model Merger V2 ç®—æ³•å®ç°

    æ–°ç‰ˆæœ¬ç‰¹ç‚¹ï¼š
    1. å…ˆå¯¹ç•™ä¸‹çš„å‚æ•°groupå¹³å‡ï¼ˆä¸è®¡ç®—è¢«ä¸¢å¼ƒçš„0å€¼ï¼‰
    2. ç„¶åå†æ•´ä½“åº”ç”¨alphaç³»æ•°
    3. åªæ”¯æŒGlobal rankingç­–ç•¥
    """

    def __init__(
        self,
        alpha: float = 0.5,
        importance_threshold: float = 0.3,
        seed: int = 42,
        **kwargs
    ):
        """
        åˆå§‹åŒ–Universal Model Merger V2

        Args:
            alpha: èåˆå¼ºåº¦ç³»æ•°ï¼Œé€‚ç”¨äºå¹³å‡åçš„ç»“æœ
            importance_threshold: é‡è¦æ€§é˜ˆå€¼ï¼Œç”¨äºå†³å®šå†²çªè§£å†³ç­–ç•¥
            seed: éšæœºç§å­
        """
        super().__init__(**kwargs)
        self.alpha = alpha
        self.importance_threshold = importance_threshold
        self.seed = seed

        # ç»Ÿè®¡ä¿¡æ¯
        self.global_stats = {
            'total_conflict_positions': 0,
            'threshold_decisions': 0,
            'random_decisions': 0,
            'per_model_stats': {}
        }

    def _compute_global_ranking(self, raw_importances: Dict[str, torch.Tensor], base_tensor: torch.Tensor, device: str):
        """Global Ranking: æ•´ä¸ªtensoræ’åºï¼Œä½¿ç”¨ [1, n_elements]"""
        ranking_importances = {}

        for model_name in self.model_names:
            flat_imp = raw_importances[model_name].flatten()
            n_elements = flat_imp.numel()

            sorted_indices = torch.argsort(flat_imp)
            ranking_scores = torch.zeros_like(flat_imp, dtype=torch.float32)
            ranking_scores[sorted_indices] = torch.arange(
                1, n_elements + 1,
                dtype=torch.float32,
                device=device
            )

            ranking_importances[model_name] = ranking_scores.view_as(base_tensor)

        return ranking_importances

    def merge_single_tensor(self, base_tensor: torch.Tensor, specialist_tensors: Dict[str, torch.Tensor]):
        """åˆå¹¶å•ä¸ªtensor - V2ç‰ˆæœ¬"""
        if not torch.is_floating_point(base_tensor):
            return base_tensor

        original_dtype = base_tensor.dtype
        device = base_tensor.device
        total_elements = base_tensor.numel()

        # æ­¥éª¤1: è®¡ç®—task vectors
        task_vecs = {}
        raw_importances = {}
        signs = {}

        for model_name in self.model_names:
            delta = specialist_tensors[model_name] - base_tensor
            task_vecs[model_name] = delta
            raw_importances[model_name] = delta.abs()
            sign = torch.sign(delta)
            signs[model_name] = torch.where(sign == 0, torch.ones_like(sign), sign)

        # æ­¥éª¤2: æ£€æµ‹å†²çª
        all_same = torch.ones_like(base_tensor, dtype=torch.bool)
        for i in range(self.n_models - 1):
            all_same = all_same & (signs[self.model_names[i]] == signs[self.model_names[i+1]])

        conflict_positions = ~all_same

        # æ­¥éª¤3: è®¡ç®—Global ranking
        ranking_importances = self._compute_global_ranking(raw_importances, base_tensor, device)

        # æ­¥éª¤4: åˆå§‹åŒ–keep masks
        keep_masks = {model_name: torch.ones_like(base_tensor, dtype=torch.bool) for model_name in self.model_names}

        # æ­¥éª¤5: å¤„ç†å†²çª
        if conflict_positions.any():
            n_conflict = conflict_positions.sum().item()
            self.global_stats['total_conflict_positions'] += n_conflict

            conflict_signs = torch.stack([
                signs[model_name][conflict_positions].flatten()
                for model_name in self.model_names
            ], dim=0)

            conflict_rankings = torch.stack([
                ranking_importances[model_name][conflict_positions].flatten()
                for model_name in self.model_names
            ], dim=0)

            is_positive = conflict_signs > 0
            is_negative = ~is_positive

            pos_ranking_sum = (conflict_rankings * is_positive.float()).sum(dim=0)
            neg_ranking_sum = (conflict_rankings * is_negative.float()).sum(dim=0)

            total_ranking = pos_ranking_sum + neg_ranking_sum
            ranking_diff_ratio = torch.abs(pos_ranking_sum - neg_ranking_sum) / (total_ranking + 1e-10)

            large_diff_mask = ranking_diff_ratio > self.importance_threshold
            small_diff_mask = ~large_diff_mask

            self.global_stats['threshold_decisions'] += large_diff_mask.sum().item()
            self.global_stats['random_decisions'] += small_diff_mask.sum().item()

            keep_decision = torch.zeros(self.n_models, n_conflict, dtype=torch.bool, device=device)

            if large_diff_mask.any():
                drop_pos = large_diff_mask & (pos_ranking_sum <= neg_ranking_sum)
                drop_neg = large_diff_mask & (pos_ranking_sum > neg_ranking_sum)
                keep_decision[:, drop_pos] = is_negative[:, drop_pos]
                keep_decision[:, drop_neg] = is_positive[:, drop_neg]

            if small_diff_mask.any():
                n_small = small_diff_mask.sum().item()
                random_choices = torch.rand(n_small, device=device) > 0.5

                random_drop_pos = torch.zeros(n_conflict, dtype=torch.bool, device=device)
                random_drop_pos[small_diff_mask] = ~random_choices
                random_drop_neg = torch.zeros(n_conflict, dtype=torch.bool, device=device)
                random_drop_neg[small_diff_mask] = random_choices

                keep_decision[:, random_drop_pos] = is_negative[:, random_drop_pos]
                keep_decision[:, random_drop_neg] = is_positive[:, random_drop_neg]

            for idx, model_name in enumerate(self.model_names):
                keep_masks[model_name][conflict_positions] = keep_decision[idx].view_as(
                    keep_masks[model_name][conflict_positions]
                )

        # æ­¥éª¤6: Rescale
        rescale_factors = {}
        for model_name in self.model_names:
            keep_count = keep_masks[model_name].sum().item()
            keep_rate = keep_count / total_elements if total_elements > 0 else 1.0
            rescale_factors[model_name] = 1.0 / keep_rate if keep_rate > 0 else 1.0

            # æ›´æ–°ç»Ÿè®¡
            if model_name not in self.global_stats['per_model_stats']:
                self.global_stats['per_model_stats'][model_name] = {'dropped': 0, 'total': 0}

            dropped = total_elements - keep_count
            self.global_stats['per_model_stats'][model_name]['dropped'] += dropped
            self.global_stats['per_model_stats'][model_name]['total'] += total_elements

        # æ­¥éª¤7: V2åˆå¹¶é€»è¾‘ - å…ˆå¹³å‡å†ä¹˜alpha
        merged_delta = torch.zeros_like(base_tensor, dtype=torch.float32)
        total_contributions = torch.zeros_like(base_tensor, dtype=torch.float32)

        # ç´¯åŠ æ‰€æœ‰éé›¶è´¡çŒ®
        for model_name in self.model_names:
            rescaled_task_vec = torch.where(
                keep_masks[model_name],
                task_vecs[model_name] * rescale_factors[model_name],
                torch.zeros_like(task_vecs[model_name])
            )
            merged_delta = merged_delta + rescaled_task_vec
            # ç»Ÿè®¡æ¯ä¸ªä½ç½®æœ‰å¤šå°‘æ¨¡å‹è´¡çŒ®
            total_contributions = total_contributions + keep_masks[model_name].float()

        # å¯¹éé›¶ä½ç½®æ±‚å¹³å‡ï¼Œé¿å…é™¤é›¶
        total_contributions = torch.clamp(total_contributions, min=1.0)
        averaged_delta = merged_delta / total_contributions

        # æœ€ååº”ç”¨alpha
        final_delta = self.alpha * averaged_delta

        return (base_tensor + final_delta).to(original_dtype)

    @torch.no_grad()
    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -> nn.Module:
        """æ‰§è¡ŒUniversal Model Merger V2ç®—æ³•"""
        if isinstance(modelpool, dict):
            modelpool = BaseModelPool(modelpool)

        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(self.seed)

        # è·å–æ¨¡å‹åç§°
        self.model_names = [name for name in modelpool.model_names if not name.startswith('_')]
        self.n_models = len(self.model_names)

        if self.n_models < 2:
            raise ValueError(f"Universal Merger V2 requires at least 2 models, got {self.n_models}")

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.global_stats['per_model_stats'] = {
            model: {'dropped': 0, 'total': 0}
            for model in self.model_names
        }

        log.info(f"ğŸš€ Universal Model Merger V2 (Global Ranking + Average-then-Scale)")
        log.info(f"Models: {self.n_models} | Threshold: {self.importance_threshold}")
        log.info(f"Model names: {self.model_names}")
        log.info(f"Alpha: {self.alpha}")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        with self.profile("load base model"):
            base_model = modelpool.load_model('_pretrained_')
            log.info("Using '_pretrained_' as base model")

        # åŠ è½½ä¸“å®¶æ¨¡å‹
        specialist_models = {}
        for model_name in self.model_names:
            with self.profile(f"load model {model_name}"):
                specialist_models[model_name] = modelpool.load_model(model_name)

        log.info("All models loaded successfully")

        # åˆå¹¶æ¨¡å‹å‚æ•°
        base_state_dict = base_model.state_dict()
        specialist_state_dicts = {name: model.state_dict() for name, model in specialist_models.items()}

        merged_state_dict = {}

        with self.profile("merge parameters"):
            for param_name in tqdm(base_state_dict.keys(), desc="Merging parameters"):
                # æ£€æŸ¥æ‰€æœ‰ä¸“å®¶æ¨¡å‹éƒ½æœ‰è¿™ä¸ªå‚æ•°
                if all(param_name in specialist_state_dicts[name] for name in self.model_names):
                    specialist_tensors = {
                        name: specialist_state_dicts[name][param_name]
                        for name in self.model_names
                    }
                    merged_state_dict[param_name] = self.merge_single_tensor(
                        base_state_dict[param_name],
                        specialist_tensors
                    )
                else:
                    # å¦‚æœæŸäº›æ¨¡å‹ç¼ºå°‘è¿™ä¸ªå‚æ•°ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å‚æ•°
                    merged_state_dict[param_name] = base_state_dict[param_name]
                    log.warning(f"Parameter {param_name} not found in all models, using base model value")

        # åŠ è½½åˆå¹¶åçš„å‚æ•°åˆ°åŸºç¡€æ¨¡å‹
        base_model.load_state_dict(merged_state_dict, strict=False)

        # æ¸…ç†å†…å­˜
        del specialist_models, specialist_state_dicts, merged_state_dict
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()

        # æ‰“å°æ€§èƒ½æŠ¥å‘Š
        self.print_profile_summary()

        log.info("Universal Model Merger V2 completed successfully")

        return base_model

    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        log.info("="*60)
        log.info("ğŸ“Š Universal Merger V2 Statistics")
        log.info("="*60)
        log.info(f"Strategy: Global Ranking + Average-then-Scale")
        log.info(f"Threshold: {self.importance_threshold}")
        log.info(f"Total conflicts: {self.global_stats['total_conflict_positions']:,}")
        log.info(f"Threshold decisions: {self.global_stats['threshold_decisions']:,}")
        log.info(f"Random decisions: {self.global_stats['random_decisions']:,}")

        log.info("\nPer-model drop rates:")
        for model in self.model_names:
            stats = self.global_stats['per_model_stats'][model]
            if stats['total'] > 0:
                drop_rate = stats['dropped'] / stats['total'] * 100
                log.info(f"  {model}: {drop_rate:.2f}% dropped ({stats['dropped']:,}/{stats['total']:,})")
        log.info("="*60)